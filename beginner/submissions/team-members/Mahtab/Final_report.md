# üü¢ Beginner Track

## ‚úÖ Week 1: Setup, EDA & Feature Engineering


### üõ†Ô∏è 1. Project Setup & Data Loading

Q: How did you set up your project environment and manage dependencies?  
A:  I created a local Python 3.12.6 virtual environment (.venv) in the project folder and selected .\.venv\Scripts\python.exe as the interpreter in VS Code and Jupyter. I installed dependencies with pip install -r requirements.txt and managed package versions using the virtual environment (pip 25.2)


Q: What steps did you take to load and inspect the dataset?  
A:
-1) I loaded  the dataset bank_full.csv using pd.read_csv() taking look the first 30 rows thorough .head(30)
-2)  using .shape to confirm the size of df (45211,17)
-3) I verified the structure with .info() which showed 7 numeric(int64) features, including ( age, balance,day,duration,campaign,pdays,previous) and 10 categorical(object) featires including (job,martial,education, housing , loan,contact,month, poutcome,y)



---

### üì¶ 2. Data Integrity & Structure

Q: Did you find any missing, duplicate, or incorrectly formatted entries in the dataset?  
-1) I did not find any missing values using isnull().sum() and additional check for unstructured placehodres(e.g., empty string,"n/a") returend 0 across all 17 column.
-2) I verified duplicated rows using .duplicated.sum() returened 0 duplicate rows.

-3) Some categorical variables (e.g., job. education,contact, putcome) included the category 'unknown', which effectively acts as a placeholder for misssing information.

Q: Are all data types appropriate for their features (e.g., numeric, categorical)?  
Yes, the datatset columns alinged with their expected types:
**##** Numeric(64):
(age, balance, day, duration, campaign,pdays,previous) all sorted as integers with no irregular formatting.
**##** Categorical(object): 
(job,martial,education, housing , loan,contact,month, poutcome,y) all stored as text labels.

---

### üìä 3. Feature Distribution & Target Assessment

Q: What did you observe about the distributions of key features (e.g., age, balance, campaign)?  
A:  

Q: Is there a class imbalance in the target variable (`y`)? How did you check for this?  
A:  

Q: What visualizations did you use to summarize your findings?  
A:  

---

### üß∞ 4. Feature Engineering

Q: What new features did you engineer (e.g., campaign frequency, time since last contact)?  
A:  

Q: Did you identify any features to exclude or transform?  
A:  

Q: How did you address class imbalance (e.g., SMOTE, class weights)?  
A:  

---

## ‚úÖ Week 2: Data Preprocessing & Model Development

---

### üè∑Ô∏è 1. Categorical Feature Encoding

Q: Which categorical features did you encode, and what encoding methods did you use (label, one-hot)?  
A:  

Q: Show a sample of the encoded data.  
A:  

---

### ‚öñÔ∏è 2. Numerical Feature Scaling

Q: Which numerical features did you scale, and which scaler did you choose (StandardScaler, MinMaxScaler)? Why?  
A:  

Q: Show summary statistics of the scaled features.  
A:  

---

### ‚úÇÔ∏è 3. Data Splitting

Q: How did you split the dataset into training, validation, and test sets? What proportions did you use?  
A:  

Q: Did you use stratification? Why or why not?  
A:  

---

### ü§ñ 4. Model Training & Evaluation

Q: Which baseline models did you train (Logistic Regression, Decision Tree, Random Forest)?  
A:  

Q: What metrics did you use to evaluate model performance?  
A:  

Q: How did you tune hyperparameters and validate your models?  
A:  

Q: Which model performed best, and why did you select it?  
A:

---

## ‚úÖ Week 3: Model Experimentation & Tracking

---

### üß™ 1. Experiment Tracking

Q: How did you track your model experiments and results?  
A:  

Q: What tools or frameworks did you use for experiment tracking (e.g., MLflow)?  
A:  

Q: How did experiment tracking help you in comparing different models and hyperparameters?  
A:  

---

### üöÄ 2. Advanced Model Training

Q: Which advanced models or boosting methods did you experiment with (e.g., XGBoost, LightGBM)?  
A:  

Q: What differences did you observe in performance compared to baseline models?  
A:  

Q: How did you handle overfitting or underfitting during experimentation?  
A:  

---

### üõ†Ô∏è 3. Hyperparameter Tuning & Validation

Q: What hyperparameter tuning strategies did you use (e.g., GridSearchCV, RandomizedSearchCV)?  
A:  

Q: How did you validate your models (e.g., cross-validation)?  
A:  

Q: What were the key hyperparameters that influenced model performance?  
A:  

---

### üìà 4. Model Selection & Insights

Q: How did you select the final model for deployment?  
A:  

Q: What metrics and business considerations influenced your decision?  
A:  

Q: What insights did you gain from the model experimentation process?  
A:
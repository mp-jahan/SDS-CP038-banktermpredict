# ğŸŸ¢ Beginner Track

## âœ… Week 1: Setup, EDA & Feature Engineering


### ğŸ› ï¸ 1. Project Setup & Data Loading

Q: How did you set up your project environment and manage dependencies?  
A:  I created a local Python 3.12.6 virtual environment (.venv) in the project folder and selected .\.venv\Scripts\python.exe as the interpreter in VS Code and Jupyter. I installed dependencies with pip install -r requirements.txt and managed package versions using the virtual environment (pip 25.2)


Q: What steps did you take to load and inspect the dataset?  
A:  

---

### ğŸ“¦ 2. Data Integrity & Structure

Q: Did you find any missing, duplicate, or incorrectly formatted entries in the dataset?  
A:  

Q: Are all data types appropriate for their features (e.g., numeric, categorical)?  
A:  

---

### ğŸ“Š 3. Feature Distribution & Target Assessment

Q: What did you observe about the distributions of key features (e.g., age, balance, campaign)?  
A:  

Q: Is there a class imbalance in the target variable (`y`)? How did you check for this?  
A:  

Q: What visualizations did you use to summarize your findings?  
A:  

---

### ğŸ§° 4. Feature Engineering

Q: What new features did you engineer (e.g., campaign frequency, time since last contact)?  
A:  

Q: Did you identify any features to exclude or transform?  
A:  

Q: How did you address class imbalance (e.g., SMOTE, class weights)?  
A:  

---

## âœ… Week 2: Data Preprocessing & Model Development

---

### ğŸ·ï¸ 1. Categorical Feature Encoding

Q: Which categorical features did you encode, and what encoding methods did you use (label, one-hot)?  
A:  

Q: Show a sample of the encoded data.  
A:  

---

### âš–ï¸ 2. Numerical Feature Scaling

Q: Which numerical features did you scale, and which scaler did you choose (StandardScaler, MinMaxScaler)? Why?  
A:  

Q: Show summary statistics of the scaled features.  
A:  

---

### âœ‚ï¸ 3. Data Splitting

Q: How did you split the dataset into training, validation, and test sets? What proportions did you use?  
A:  

Q: Did you use stratification? Why or why not?  
A:  

---

### ğŸ¤– 4. Model Training & Evaluation

Q: Which baseline models did you train (Logistic Regression, Decision Tree, Random Forest)?  
A:  

Q: What metrics did you use to evaluate model performance?  
A:  

Q: How did you tune hyperparameters and validate your models?  
A:  

Q: Which model performed best, and why did you select it?  
A:

---

## âœ… Week 3: Model Experimentation & Tracking

---

### ğŸ§ª 1. Experiment Tracking

Q: How did you track your model experiments and results?  
A:  

Q: What tools or frameworks did you use for experiment tracking (e.g., MLflow)?  
A:  

Q: How did experiment tracking help you in comparing different models and hyperparameters?  
A:  

---

### ğŸš€ 2. Advanced Model Training

Q: Which advanced models or boosting methods did you experiment with (e.g., XGBoost, LightGBM)?  
A:  

Q: What differences did you observe in performance compared to baseline models?  
A:  

Q: How did you handle overfitting or underfitting during experimentation?  
A:  

---

### ğŸ› ï¸ 3. Hyperparameter Tuning & Validation

Q: What hyperparameter tuning strategies did you use (e.g., GridSearchCV, RandomizedSearchCV)?  
A:  

Q: How did you validate your models (e.g., cross-validation)?  
A:  

Q: What were the key hyperparameters that influenced model performance?  
A:  

---

### ğŸ“ˆ 4. Model Selection & Insights

Q: How did you select the final model for deployment?  
A:  

Q: What metrics and business considerations influenced your decision?  
A:  

Q: What insights did you gain from the model experimentation process?  
A: